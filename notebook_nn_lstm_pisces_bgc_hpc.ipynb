{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e97f90f",
   "metadata": {},
   "source": [
    "# Load all the libraries\n",
    "\n",
    "\n",
    "**Import required libraries and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #, category=UserWarning)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_notebook = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd93165",
   "metadata": {},
   "source": [
    "# Define the path of the data\n",
    "\n",
    "\n",
    "**Import required libraries and modules**\n",
    "\n",
    "- The directory is mounted from the **bscearth** server\n",
    "- Experiment: EC-Earth **A5CW** \n",
    "- CMORFILES\n",
    "- piControl; Pre-Industrial (pi)\n",
    "- Spatial Resolution: **r1i1p1f1**\n",
    "- Model: Ocean Only **Omon**\n",
    "- Temporal: **Monthly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpfs = '/gpfs/projects/bsc32/bsc032196'\n",
    "list_dir = gpfs + '/esarchive/exp/ecearth/a5cw/original_files/cmorfiles/CMIP/EC-Earth-Consortium/EC-Earth3-CC/piControl/r1i1p1f1/Omon/'\n",
    "print(\"MAIN DIRECTORY: \",list_dir)\n",
    "\n",
    "pisces_files = os.listdir(list_dir)\n",
    "print(\"\")                   \n",
    "print(\"List of subdirectories (Variables): \",pisces_files)\n",
    "\n",
    "\n",
    "\n",
    "parent_dir = Path().resolve().parent\n",
    "print(\"Parent Directory:\", parent_dir)\n",
    "\n",
    "epoch_range = '*gn_19*' #*gn_199* 1990s #*gn_199* 1990s \n",
    "file_ext = '.nc'\n",
    "if os.path.exists(f'{parent_dir}/plots/Heatmap_{epoch_range}') == False:\n",
    "    os.makedirs(f'{parent_dir}/plots/Heatmap_{epoch_range}', exist_ok=True) \n",
    "if os.path.exists(f'{parent_dir}/output_time/Heatmap_{epoch_range}') == False:\n",
    "    os.makedirs(f'{parent_dir}/output_time/Heatmap_{epoch_range}', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96c63d-cfaa-4278-ba4b-c9e24e44b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_dir)\n",
    "print(pisces_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547636b6",
   "metadata": {},
   "source": [
    "# Extract the variables\n",
    "\n",
    "- Z(t) = INTPP : net_primary_mole_productivity_of_biomass_expressed_as_carbon_by_phytoplankton [mol m-2 s-1]\n",
    "- Z(t-1) = INTPP Lag 1: \n",
    "- A(t) = NO3 3D (Column integrate 200m): mole_concentration_of_nitrate_in_sea_water [mol m-3]\n",
    "- B(t) = PO4 3D (Column integrate 200m): mole_concentration_of_dissolved_inorganic_phosphorus_in_sea_water [mol m-3]\n",
    "- C(t) = Si 3D (*Column integrate 200m*)\n",
    "- D(t) = THETAO 3D (Column integrate 200m): sea_water_potential_temperature [degC]\n",
    "- E(t) = MLOTST: ocean_mixed_layer_thickness_defined_by_sigma_t [m]\n",
    "- G(t) = DFE 3D (Column integrate 200m): mole_concentration_of_dissolved_iron_in_sea_water [mol m-3]\n",
    "\n",
    "- H(t) = ZMESOOS (Surface) Only from 1959..onwards: Surface Mole Concentration of Mesoozooplankton expressed as Carbon in Sea Water [mol m-3]\n",
    "- I(t) = ZMICROOS (Surface) Only from 1959..onwards: Microzooplankton [mol m-3]\n",
    "- *J(t) = ZMISC ?*\n",
    "- *K(t) = PHOT (Column integrate 200m)?*\n",
    "- L(t) = RSNTDS (Surface): Net Surface Downward Shortwave Radiation at Surface [W m-2] \n",
    "- M(t) = UMO (at 50 m): ocean_mass_x_transport [kg s-1]\n",
    "- O(t) = VMO (at 50 m): ocean_mass_y_transport [kg s-1]\n",
    "- P(t) = WMO (at 50 m): upward_ocean_mass_transport [kg s-1]\n",
    "\n",
    "\n",
    "Multivar autoregressive model with INTPP vs. 6 external variables \n",
    "\n",
    "Z(t) = z*Z(t-1) + a*A(t)....e*E(t)....l*L(t)\n",
    "\n",
    "ARIMA:  with (p,d,q) Model\n",
    "\n",
    "LSTM: Long Short-Term Memory\n",
    "\n",
    "Prediction for PISCES variables\n",
    "Z(t) = Z(t-1) + A....L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c072f",
   "metadata": {},
   "source": [
    "**Start the wall clock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b2a0b-df39-43ec-b436-636b1ae91811",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_load = time.time()\n",
    "\n",
    "# Print the variables and their positions in the list\n",
    "list_multivar = ['INTPP','NO3' ,'PO4','SI','THETAO','MLOTST','DFE','ZMESOOS','ZMICROOS',\n",
    "                 'RSNTDS','VMO','WMO','UMO']\n",
    "#list_multivar = ['INTPP','NO3', 'PO4'] #,'VMO','WMO','ZMESOOS','ZMICROOS','NO3','UMO','PO4','SI','DFE','MLOTST','RSNTDS','THETAO']\n",
    "#list_multivar = ['INTPP','DFE','RSNTDS']\n",
    "\n",
    "\n",
    "epoch = epoch_range + file_ext\n",
    "\n",
    "for i in range(len(pisces_files)):\n",
    "    \n",
    "    if pisces_files[i].upper() in list_multivar:\n",
    "        \n",
    "        \n",
    "        #os.environ[\"HDF5_DISABLE_VERSION_CHECK\"] = \"2\"\n",
    "        warnings.filterwarnings('ignore' , category=UserWarning)\n",
    "        files = list_dir+pisces_files[i]+'/gn/v20221230/'\n",
    "        #print(\"Variable Directory \", files)\n",
    "        list_nc_files = os.listdir(files)\n",
    "        #print(\"All Files for this VARIABLE\",list_nc_files)\n",
    "        if pisces_files[i].upper() == 'INTPP':\n",
    "            print(\"Position, VARIABLE:  \",i,pisces_files[i].upper())\n",
    "            intpp = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "            #intpp = xr.open_mfdataset(files+epoch).isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'NO3':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            no3 = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'PO4':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            po4 = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'SI':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            si = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'THETAO':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            theta0 = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'MLOTST':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            mlotst = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'DFE':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            dfe = xr.open_mfdataset(files+epoch,combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'ZMESOOS':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            zmesoos = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'ZMICROOS':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            zmicroos = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'RSNTDS':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            rsntds = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'WMO':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            wmo = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'UMO':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            umo = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "        if pisces_files[i].upper() == 'VMO':\n",
    "            print(\"VARIABLE DIR:  \",i,pisces_files[i].upper())\n",
    "            vmo = xr.open_mfdataset(files+epoch, combine='by_coords')#.isel(time=slice(None, None, 12))\n",
    "\n",
    "\n",
    "                    \n",
    "end_time_load = time.time()\n",
    "time_used_load = (end_time_load - start_time_load)\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load:.4f}', \"Seconds\")\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/60:.4f}', \"Minutes\")\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/3600:.4f}', \"Hours\")\n",
    "# Open a file in write mode\n",
    "with open(f\"{parent_dir}/output_time/Heatmap_{epoch_range}/output_time_heatmap_{epoch_range}_load.txt\", \"w\") as file:\n",
    "    # Redirect print output to the file\n",
    "    print(\"Time used for LOAD..../zzzz\", f'{time_used_load:.4f}', \"Seconds\", file=file)\n",
    "    print(\"Time used for LOAD..../zzzz\", f'{time_used_load/60:.4f}', \"Minutes\", file=file)\n",
    "    print(\"Time used for LOAD..../zzzz\", f'{time_used_load/3600:.4f}', \"Hours\", file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109dc1f",
   "metadata": {},
   "source": [
    "# Time Series Extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr --no-display\n",
    "warnings.filterwarnings('ignore' , category=UserWarning)\n",
    "\n",
    "class TimeSeriesExtractor:\n",
    "    def __init__(self, datasets, k_factors, train_start_year,\n",
    "                 train_end_year, test_start_year, test_end_year, geo_locations):\n",
    "        self.datasets = datasets  # A dictionary of datasets\n",
    "        self.k_factors = k_factors  # Dictionary of scaling factors for each variable\n",
    "        self.train_start_year = train_start_year\n",
    "        self.train_end_year = train_end_year\n",
    "        self.test_start_year = test_start_year\n",
    "        self.test_end_year = test_end_year\n",
    "        self.geo_locations = geo_locations\n",
    "\n",
    "    def extract_time_series(self, var_name, i_index, j_index, lev=None, method='nearest'):\n",
    "        \"\"\"\n",
    "        Extracts time series data from the specified dataset at the given indices.\n",
    "        \n",
    "        :param var_name: The variable name in the dataset.\n",
    "        :param i_index: The i index of the target point.\n",
    "        :param j_index: The j index of the target point.\n",
    "        :param lev: Optional level index for 3D variables.\n",
    "        :param method: The method to use for selection ('nearest' by default).\n",
    "        :return: The extracted time series.\n",
    "        \"\"\"\n",
    "        warnings.filterwarnings('ignore' , category=UserWarning)\n",
    "        \n",
    "        print(\"LEVEL??\",lev)\n",
    "        if lev is not None:\n",
    "            \n",
    "            lev_end = lev\n",
    "            lev_start = 0\n",
    "            print(\"From 0 to LEVEL??\",lev_start,lev_end)\n",
    "            time_series = self.datasets['data'][var_name].isel(i=i_index, j=j_index,lev=slice(lev_start, lev_end + 1)).mean(dim='lev')\n",
    "        else:\n",
    "            #print(\"self.datasets['data'][var_name].isel(i=i_index, j=j_index)\",\n",
    "            #      self.datasets['data'][var_name].isel(i=i_index, j=j_index))\n",
    "            time_series = self.datasets['data'][var_name].isel(i=i_index, j=j_index)\n",
    "        \n",
    "        return time_series[var_name]\n",
    "\n",
    "    def normalize_time_series(self, time_series):\n",
    "        \"\"\"\n",
    "        Normalizes a given time series array to have zero mean and unit variance.\n",
    "        \n",
    "        :param time_series: The time series data array to normalize.\n",
    "        :return: The normalized time series data array.\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"time_series\",time_series)\n",
    "        #print(\"type(time_series)\",type(time_series))\n",
    "        mean = np.mean(time_series)\n",
    "        std = np.std(time_series)\n",
    "        normalized_series = (time_series - mean) / (std + 0.01)\n",
    "        return normalized_series\n",
    "\n",
    "    def k_scaled_timeseries(self, time_series, k_factor):\n",
    "        \"\"\"\n",
    "        Transform with k_factor multiplication, addition with constant, \n",
    "        logarithmic transformation and by adding a constant value of 12\n",
    "        \n",
    "        :param time_series: The time series data array.\n",
    "        :return: The k-scaled time series data array.\n",
    "        \"\"\"\n",
    "        k_scaled_series = np.log1p(np.abs(time_series) * k_factor + np.exp(2)) + 12\n",
    "        return k_scaled_series\n",
    "\n",
    "    def replace_nan_with_zero(self, time_series):\n",
    "        \"\"\"\n",
    "        Checks for NaN values in the time series and replaces them with 0.\n",
    "        \n",
    "        :param time_series: The time series data array.\n",
    "        :return: The time series data array with NaNs replaced by 0.\n",
    "        \"\"\"\n",
    "        time_series[np.isnan(time_series)] = 0\n",
    "        return time_series\n",
    "\n",
    "    def get_time_series_for_multiple_points(self, index_pairs):\n",
    "        \"\"\"\n",
    "        Extracts time series data for multiple points specified by index pairs.\n",
    "        \n",
    "        :param index_pairs: A list of (i_index, j_index) pairs.\n",
    "        :return: A dictionary with keys as (i_index, j_index) tuples and values as time series data dictionaries.\n",
    "        \"\"\"\n",
    "        all_time_series_data = {}\n",
    "        for (i_index, j_index) in index_pairs:\n",
    "            print(\"Geo-location: \", i_index, j_index)\n",
    "            print(\"self.geo_locations['geo_points']:\", self.geo_locations['geo_points'].items())\n",
    "            print(\"self.geo_locations['geo_points']:\", self.geo_locations['geo_points'].keys())\n",
    "            print(\"f'({i_index},{j_index})'\",f'({i_index}, {j_index})')\n",
    "            print(\"get.geo_locations['geo_points'](f'({i_index}, {j_index})')\",self.geo_locations['geo_points'][(i_index, j_index)])\n",
    "            time_series_data = {}\n",
    "            for key, variable in self.datasets['variables'].items():\n",
    "                lev = variable.get('lev', None)\n",
    "                print(\"variable['name'] \",variable['name'])\n",
    "                time_series = self.extract_time_series(variable['name'], i_index, j_index, lev).values\n",
    "                \n",
    "                # Replace NaN values with 0\n",
    "                time_series = self.replace_nan_with_zero(time_series)\n",
    "                \n",
    "                # Normalize time series\n",
    "                normalized_series = self.normalize_time_series(time_series)\n",
    "                \n",
    "                # Apply k factor scaling\n",
    "                k_factor = self.k_factors.get(key, 1)\n",
    "                k_scaled_series = self.k_scaled_timeseries(time_series, k_factor)\n",
    "                \n",
    "                # Replace NaN values with 0 again after scaling\n",
    "                normalized_series = self.replace_nan_with_zero(normalized_series)\n",
    "                k_scaled_series = self.replace_nan_with_zero(k_scaled_series)\n",
    "                \n",
    "                # Split the data into training and testing based on specified years\n",
    "                train_mask = (self.datasets['data'][variable['name']].time.dt.year >= self.train_start_year) & (self.datasets['data'][variable['name']].time.dt.year <= self.train_end_year)\n",
    "                test_mask = (self.datasets['data'][variable['name']].time.dt.year >= self.test_start_year) & (self.datasets['data'][variable['name']].time.dt.year <= self.test_end_year)\n",
    "                \n",
    "                time_series_data[key] = {\n",
    "                    'normalized_train': normalized_series[train_mask],\n",
    "                    'normalized_test': normalized_series[test_mask],\n",
    "                    'k_scaled_train': k_scaled_series[train_mask],\n",
    "                    'k_scaled_test': k_scaled_series[test_mask]\n",
    "                }\n",
    "            all_time_series_data[(i_index, j_index)] = time_series_data\n",
    "        return all_time_series_data\n",
    "\n",
    "    def time_function_execution(self, func, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Measures the wall clock time taken by a function.\n",
    "        \n",
    "        :param func: The function to execute.\n",
    "        :param args: Positional arguments to pass to the function.\n",
    "        :param kwargs: Keyword arguments to pass to the function.\n",
    "        :return: The result of the function and the time taken (in seconds).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        return result, elapsed_time\n",
    "\n",
    "    def convert_to_dataframe(self, time_series_data):\n",
    "        \"\"\"\n",
    "        Converts the time series data to pandas DataFrames.\n",
    "\n",
    "        :param time_series_data: The dictionary containing the time series data.\n",
    "        :return: Two pandas DataFrames for training and testing data.\n",
    "        \"\"\"\n",
    "        df_train = pd.DataFrame()\n",
    "        df_test = pd.DataFrame()\n",
    "\n",
    "        # Use the time index from the dataset\n",
    "        for point_key, series_dict in time_series_data.items():\n",
    "            for variable_key, series_data in series_dict.items():\n",
    "                train_col_names = {\n",
    "                    'normalized': f\"{variable_key}_normalized_train_{point_key}\",\n",
    "                    'k_scaled': f\"{variable_key}_k_scaled_train_{point_key}\"\n",
    "                }\n",
    "                test_col_names = {\n",
    "                    'normalized': f\"{variable_key}_normalized_test_{point_key}\",\n",
    "                    'k_scaled': f\"{variable_key}_k_scaled_test_{point_key}\"\n",
    "                }\n",
    "                \n",
    "                df_train[train_col_names['normalized']] = series_data['normalized_train']\n",
    "                df_test[test_col_names['normalized']] = series_data['normalized_test']\n",
    "                df_train[train_col_names['k_scaled']] = series_data['k_scaled_train']\n",
    "                df_test[test_col_names['k_scaled']] = series_data['k_scaled_test']\n",
    "\n",
    "        # Assign the date range as the index of the DataFrame\n",
    "        time_train_index = self.datasets['data'][variable_key].time.sel(time=(self.datasets['data'][variable_key].time.dt.year >= self.train_start_year) & (self.datasets['data'][variable_key].time.dt.year <= self.train_end_year))\n",
    "        time_test_index = self.datasets['data'][variable_key].time.sel(time=(self.datasets['data'][variable_key].time.dt.year >= self.test_start_year) & (self.datasets['data'][variable_key].time.dt.year <= self.test_end_year))\n",
    "        \n",
    "        df_train.index = time_train_index\n",
    "        df_test.index = time_test_index\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "    def add_lagged_series(self, df_train, df_test, variable, point):\n",
    "        \"\"\"\n",
    "        Adds a lag-1 shifted series for the specified variable and point.\n",
    "\n",
    "        :param df_train: The training DataFrame.\n",
    "        :param df_test: The testing DataFrame.\n",
    "        :param variable: The variable for which to add the lagged series.\n",
    "        :param point: The point (i, j) for which to add the lagged series.\n",
    "        \"\"\"\n",
    "        point_key = point\n",
    "        variable_key = variable\n",
    "\n",
    "        train_col = {\n",
    "            'normalized': f\"{variable_key}_normalized_train_{point_key}\",\n",
    "            'k_scaled': f\"{variable_key}_k_scaled_train_{point_key}\"\n",
    "        }\n",
    "        test_col = {\n",
    "            'normalized': f\"{variable_key}_normalized_test_{point_key}\",\n",
    "            'k_scaled': f\"{variable_key}_k_scaled_test_{point_key}\"\n",
    "        }\n",
    "        \n",
    "        lagged_column_train = {\n",
    "            'normalized': f\"{variable_key}_lag1_normalized_train_{point_key}\",\n",
    "            'k_scaled': f\"{variable_key}_lag1_k_scaled_train_{point_key}\"\n",
    "        }\n",
    "        lagged_column_test = {\n",
    "            'normalized': f\"{variable_key}_lag1_normalized_test_{point_key}\",\n",
    "            'k_scaled': f\"{variable_key}_lag1_k_scaled_test_{point_key}\"\n",
    "        }\n",
    "        \n",
    "        df_train[lagged_column_train['normalized']] = df_train[train_col['normalized']].shift(1)\n",
    "        df_test[lagged_column_test['normalized']] = df_test[test_col['normalized']].shift(1)\n",
    "        df_train[lagged_column_train['k_scaled']] = df_train[train_col['k_scaled']].shift(1)\n",
    "        df_test[lagged_column_test['k_scaled']] = df_test[test_col['k_scaled']].shift(1)\n",
    "\n",
    "        df_train[lagged_column_train['normalized']].iloc[0] = np.nan  # First value is NaN\n",
    "        df_test[lagged_column_test['normalized']].iloc[0] = df_train[lagged_column_train['normalized']].iloc[-1]  # Initial test value is the last train value\n",
    "        df_train[lagged_column_train['k_scaled']].iloc[0] = np.nan  # First value is NaN\n",
    "        df_test[lagged_column_test['k_scaled']].iloc[0] = df_train[lagged_column_train['k_scaled']].iloc[-1]  # Initial test value is the last train value\n",
    "\n",
    "    def apply_cross_correlation(self, series1, series2, max_lag=3):\n",
    "        series1 = np.array(series1)\n",
    "        series2 = np.array(series2)\n",
    "        cross_corr = {}\n",
    "\n",
    "        for lag in range(-max_lag, max_lag + 1):\n",
    "            if lag < 0:\n",
    "                lagged_series1 = series1[:lag]\n",
    "                lagged_series2 = series2[-lag:]\n",
    "            elif lag > 0:\n",
    "                lagged_series1 = series1[lag:]\n",
    "                lagged_series2 = series2[:-lag]\n",
    "            else:\n",
    "                lagged_series1 = series1\n",
    "                lagged_series2 = series2\n",
    "\n",
    "            corr = scipy.stats.pearsonr(lagged_series1,lagged_series2)\n",
    "            #corr = np.correlate(lagged_series1 - np.mean(lagged_series1), lagged_series2 - np.mean(lagged_series2), mode='valid')\n",
    "            #norm_factor = np.std(lagged_series1) * np.std(lagged_series2)\n",
    "            cross_corr[lag] = corr.statistic\n",
    "            #if norm_factor > 0:\n",
    "            #    cross_corr[lag] = corr[0] / norm_factor\n",
    "            #else:\n",
    "            #    cross_corr[lag] = np.nan\n",
    "        \n",
    "        return cross_corr\n",
    "\n",
    "    def calculate_cross_correlations(self, time_series_data, max_lag=3):\n",
    "        results = []\n",
    "\n",
    "        for point_key, series_dict in time_series_data.items():\n",
    "            for var1_key, series1_data in series_dict.items():\n",
    "                for var2_key, series2_data in series_dict.items():\n",
    "                    if var1_key != var2_key:  # Avoid self-correlation\n",
    "                        cross_corr = self.apply_cross_correlation(series1_data['normalized_train'], series2_data['normalized_train'], max_lag)\n",
    "                        for lag, corr_value in cross_corr.items():\n",
    "                            results.append({\n",
    "                                'point': point_key,\n",
    "                                'geopoint':self.geo_locations['geo_points'][point_key],\n",
    "                                'var1': var1_key,\n",
    "                                'var2': var2_key,\n",
    "                                'lag': lag,\n",
    "                                'correlation': corr_value\n",
    "                            })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "datasets = {\n",
    "    'data': {\n",
    "        'intpp': intpp,\n",
    "        'no3': no3,\n",
    "        'po4': po4,\n",
    "        'si': si,\n",
    "        'thetao': theta0,\n",
    "        'mlotst': mlotst,\n",
    "        'dfe': dfe,\n",
    "        'zmicroos': zmicroos,\n",
    "        'zmesoos': zmesoos,\n",
    "        'rsntds': rsntds,\n",
    "        'umo': umo,\n",
    "        'vmo': vmo,\n",
    "        'wmo': wmo\n",
    "        \n",
    "    },\n",
    "    \n",
    "    'variables': {\n",
    "        'intpp': {'name': 'intpp'},\n",
    "        'no3': {'name': 'no3', 'lev': 6},\n",
    "        'po4': {'name': 'po4', 'lev': 6},\n",
    "        'si': {'name': 'si', 'lev': 6},\n",
    "        'thetao': {'name': 'thetao', 'lev': 6},\n",
    "        'mlotst': {'name': 'mlotst'},\n",
    "        'dfe': {'name': 'dfe', 'lev': 6},\n",
    "        'zmicroos': {'name': 'zmicroos'},\n",
    "        'zmesoos': {'name': 'zmesoos'},\n",
    "        'rsntds': {'name': 'rsntds'},\n",
    "        'umo': {'name': 'umo', 'lev': 6},\n",
    "        'vmo': {'name': 'vmo', 'lev': 6},\n",
    "        'wmo': {'name': 'wmo', 'lev': 6},\n",
    "    }\n",
    "}\n",
    "\n",
    "k_factors = {\n",
    "    'intpp': 5E9,\n",
    "    'no3': 5E3,\n",
    "    'po4': 5E4,\n",
    "    'si': 3E3,\n",
    "    'thetao': 2E1,\n",
    "    'mlotst': 1,\n",
    "    'dfe': 3E7,\n",
    "    'zmicroos': 2E4,\n",
    "    'zmesoos': 1E5,\n",
    "    'rsntds': 3E-2,\n",
    "    'umo': 1E-8,\n",
    "    'vmo': 1E-8,\n",
    "    'wmo': 1E-8,\n",
    "}\n",
    "\n",
    "\n",
    "geo_locations = {\n",
    "    'geo_points': {\n",
    "        (120, 290): 'NORTH. POLAR PACIF.',\n",
    "        (150, 150): 'TROP. PACIF.',\n",
    "        (200, 150): 'GALAPAGOS',\n",
    "        (250, 200): 'SUBTRP. NOR. ATLNT.',\n",
    "        (100, 200): 'NORTH. PACIF.',\n",
    "        (210, 250): 'LABRADOR',\n",
    "        (350, 150): 'INDIAN',\n",
    "        (50, 70): 'SO. AUSTRAL.',\n",
    "        (150, 100): 'SO. PACIF.',\n",
    "        (350, 100): 'SO. INDIA',\n",
    "        (300, 70): 'SO. ATLANTIC',\n",
    "    }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef1acb-35d8-4397-b7f1-2118a262ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr --no-display\n",
    "\n",
    "start_time_extractor = time.time()\n",
    "\n",
    "# Instantiate the class\n",
    "warnings.filterwarnings('ignore')\n",
    "extractor = TimeSeriesExtractor(datasets, k_factors,1959,1989,1990,1999,geo_locations)\n",
    "warnings.filterwarnings('ignore' , category=UserWarning)\n",
    "print(\"extractor \",extractor)\n",
    "# List of index pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Measure time taken to extract and normalize time series\n",
    "normalized_time_series, execution_time = extractor.time_function_execution(extractor.get_time_series_for_multiple_points, index_pairs)\n",
    "\n",
    "# Convert the time series data to DataFrames\n",
    "df_train, df_test = extractor.convert_to_dataframe(normalized_time_series)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "print(df_train.tail())\n",
    "print(df_test.tail())\n",
    "\n",
    "# After calculating cross-correlations\n",
    "cross_corr_df = extractor.calculate_cross_correlations(normalized_time_series, max_lag=3)\n",
    "\n",
    "print(\"cross_corr_df.head()\",cross_corr_df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "cross_corr_df.to_csv('cross_correlation_results.csv', index=False)\n",
    "\n",
    "\n",
    "end_time_extractor = time.time()\n",
    "time_used_extractor = (end_time_extractor - start_time_extractor)\n",
    "print(\"Time used for Extractor Class TimeSeriesExtractor(datasets, k_factors) ..../zzzz\", f'{time_used_extractor:.4f}', \"Seconds\")\n",
    "print(\"Time used for Extractor Class TimeSeriesExtractor(datasets, k_factors)..../zzzz\", f'{time_used_extractor/60:.4f}', \"Minutes\")\n",
    "print(\"Time used for Extractor Class TimeSeriesExtractor(datasets, k_factors)..../zzzz\", f'{time_used_extractor/3600:.4f}', \"Hours\")\n",
    "\n",
    "# Open a file in write mode\n",
    "with open(f\"{parent_dir}/output_time/Heatmap_{epoch_range}/output_time_heatmap_{epoch_range}_load.txt\", \"w\") as file:\n",
    "    # Redirect print output to the file\n",
    "    print(\"Time used for LOAD..../zzzz\", f'{time_used_extractor:.4f}', \"Seconds\", file=file)\n",
    "    print(\"Time used for LOAD..../zzzz\", f'{time_used_extractor/60:.4f}', \"Minutes\", file=file)\n",
    "    print(\"Time used for LOAD..../zzzz\", f'{time_used_extractor/3600:.4f}', \"Hours\", file=file)\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3157b-b09b-47b0-a5bc-044bff8b6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture --no-stderr --no-display\n",
    "# Load the data\n",
    "df = cross_corr_df\n",
    "\n",
    "def plot_correlations(df):\n",
    "    # Create a unique list of (point, var1, var2) combinations\n",
    "    unique_combinations = df[['point', 'var1', 'var2']].drop_duplicates()\n",
    "\n",
    "    # Iterate over each unique (point, var1, var2) combination and create a separate plot\n",
    "    for index, row in unique_combinations.iterrows():\n",
    "        point, var1, var2 = row['point'], row['var1'], row['var2']\n",
    "        \n",
    "        # Filter data for the specific combination\n",
    "        subset = df[(df['point'] == point) & (df['var1'] == var1) & (df['var2'] == var2)]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(subset['lag'], subset['correlation'], marker='o', linestyle='-', label=f'{var1} vs {var2}', color=plt.cm.tab10(index % 10))\n",
    "        plt.xlabel('Lag')\n",
    "        plt.ylabel('Correlation')\n",
    "        plt.title(f'Correlation between {var1} and {var2} at {point}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the figure\n",
    "        filename = f\"{parent_dir}/plots/heatmap_correlation_plot_{point}_{var1}_vs_{var2}.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "# Run the function to generate and save the plots\n",
    "plot_correlations(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50459bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = index_pairs\n",
    "\n",
    "for area in area_dict:\n",
    "    extractor.add_lagged_series(df_train, df_test, 'intpp', area)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "291ef3da",
   "metadata": {},
   "source": [
    "# Add lagged series for the 'intpp' variable at the (120, 290) point\n",
    "# Instantiate the class\n",
    "extractor = TimeSeriesExtractor(datasets, k_factors,1959,2010,2011,2021)\n",
    "area_dict = index_pairs\n",
    "for area in area_dict:\n",
    "    extractor.add_lagged_series(df_train, df_test, 'intpp', area)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d93f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef390496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb75769c",
   "metadata": {},
   "source": [
    "#df_test['I_tst'] = df_test['I_tst'].apply(lambda x: np.sqrt(x)) \n",
    "#df_test['G_tst'] = df_test['G_tst'].apply(lambda x: np.sqrt(x))\n",
    "#df_test['H_tst'] = df_test['H_tst'].apply(lambda x: np.sqrt(x))\n",
    "\n",
    "#df_train['I_trn'] = df_train['I_trn'].apply(lambda x: np.sqrt(x)) \n",
    "#df_train['H_trn'] = df_train['H_trn'].apply(lambda x: np.sqrt(x)) \n",
    "#df_train['G_trn'] = df_train['G_trn'].apply(lambda x: np.sqrt(x))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac4c2361",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cftime\n",
    "\n",
    "# Function to convert cftime to datetime\n",
    "def convert_cftime_to_datetime(index):\n",
    "    return [pd.Timestamp(date.year, date.month, date.day, date.hour, date.minute, date.second) for date in index]\n",
    "\n",
    "# Example DataFrames\n",
    "# Assuming df_train and df_test are defined with cftime index\n",
    "# df_train = ...\n",
    "# df_test = ...\n",
    "\n",
    "# Convert the index of both DataFrames\n",
    "df_train.index = convert_cftime_to_datetime(df_train.index)\n",
    "df_test.index = convert_cftime_to_datetime(df_test.index)\n",
    "\n",
    "# Function to create the mapping dictionary\n",
    "def create_mapping(train_columns):\n",
    "    mapping = {}\n",
    "    for column in train_columns:\n",
    "        # Create corresponding test column name by replacing 'train' with 'test'\n",
    "        test_column = column.replace('train', 'test')\n",
    "        mapping[column] = test_column\n",
    "    return mapping\n",
    "\n",
    "# Create the mapping dictionary\n",
    "column_mapping = create_mapping(df_train.columns)\n",
    "\n",
    "# Function to plot subplots in chunks\n",
    "def plot_in_chunks(df_train, df_test, column_mapping, chunk_size=10):\n",
    "    columns = df_train.columns\n",
    "    num_columns = len(columns)\n",
    "    num_chunks = (num_columns + chunk_size - 1) // chunk_size  # Calculate number of chunks\n",
    "\n",
    "    for chunk in range(num_chunks):\n",
    "        start = chunk * chunk_size\n",
    "        end = min(start + chunk_size, num_columns)\n",
    "        chunk_columns = columns[start:end]\n",
    "\n",
    "        fig, axes = plt.subplots(len(chunk_columns), 1, figsize=(10, 6 * len(chunk_columns)), sharex=True)\n",
    "        \n",
    "        if len(chunk_columns) == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable even for a single subplot\n",
    "\n",
    "        for i, column in enumerate(chunk_columns):\n",
    "            axes[i].plot(df_train.index, df_train[column], label=f'Train {column}')\n",
    "            test_column = column_mapping.get(column)\n",
    "            if test_column and test_column in df_test.columns:\n",
    "                axes[i].plot(df_test.index, df_test[test_column], label=f'Test {test_column}')\n",
    "            else:\n",
    "                print(f\"Warning: Corresponding test column for '{column}' not found in test DataFrame.\")\n",
    "            axes[i].set_title(f'Time Series Data for {column}')\n",
    "            axes[i].set_xlabel('Date')\n",
    "            axes[i].set_ylabel('Value')\n",
    "            axes[i].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if os.path.exists('plots/TRAIN_TEST_DATA/') == False:\n",
    "            !mkdir 'plots/TRAIN_TEST_DATA/'\n",
    "\n",
    "        plt.savefig(f'plots/TRAIN_TEST_DATA/TRAIN_TEST_SERIES_{column}_{chunk}.png') \n",
    "        #plt.show()\n",
    "\n",
    "# Plot the data in chunks\n",
    "plot_in_chunks(df_train, df_test, column_mapping, chunk_size=10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "754dff41",
   "metadata": {},
   "source": [
    "normx_dict = ['k_scaled','normalized'] #['raw','normalized','min_max','log','k_scaled']\n",
    "# Different areas\n",
    "area_dict = index_pairs\n",
    "print(index_pairs)\n",
    "for normx in normx_dict:\n",
    "        for area in area_dict:\n",
    "            \n",
    "            print(f'ARIMA LOOP  NORM:{normx}, AREA @ {area}')\n",
    "           \n",
    "            area = str(area)\n",
    "            exog_df = df_train[['intpp_lag1_'+normx+'_train_'+area,\n",
    "                                'no3_'+normx+'_train_'+area,\n",
    "                                'po4_'+normx+'_train_'+area]]\n",
    "                                #'si_'+normx+'_train_'+area,\n",
    "                               #'thetao_'+normx+'_train_'+area,\n",
    "                               #'mlotst_'+normx+'_train_'+area,\n",
    "                                #'dfe_'+normx+'_train_'+area,\n",
    "                                #'zmicroos_'+normx+'_train_'+area,\n",
    "                               #'zmesoos_'+normx+'_train_'+area,\n",
    "                                #'rsntds_'+normx+'_train_'+area]]\n",
    "            \n",
    "            \n",
    "print(exog_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cross_correlation(series1, series2):\n",
    "        series1 = np.array(series1)\n",
    "        series2 = np.array(series2)\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "        corr = scipy.stats.pearsonr(series1,series2)\n",
    "        cross_corr = corr.statistic\n",
    "      \n",
    "        \n",
    "        return cross_corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0ac27",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "**Using the Long Short Term Memory for prediction** \n",
    "- Define the LSTM model architecture\n",
    "- model = Sequential() For timeseries the Sequential Keras class is suitable\n",
    "- model.add(LSTM(64, input_shape=(time_steps, 1))) Add LSTM cell into the Sequential architectur\n",
    "- model.add(Dropout(0.2))  # Add dropout layer with a dropout rate of 0.2\n",
    "- model.add(Dense(1)) Predict the next step of the time series\n",
    "- model.compile(optimizer='adam', loss='mean_squared_error') At the end the model will be compiled with an Adam optimizer (For Gradient descent to reach the cost function) and the loss (cost function) of Mean Square Error (Similar to Least Square Fit).\n",
    "\n",
    "# Train the model\n",
    " - model.fit(X_train, y_train, epochs=50, batch_size=16) As model is compiled, then a fit can be performed on the training data set with 50 epochs (runs for convergence) taking 16 data points as batches (at once) for running moving averags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "start_time_lstm = time.time()\n",
    "\n",
    "\"\"\"intpp_raw_train_(120, 290)\n",
    "intpp_normalized_train_(120, 290)\n",
    "intpp_min_max_train_(120, 290)\n",
    "intpp_log_train_(120, 290)\n",
    "intpp_k_scaled_train_(120, 290)\n",
    "no3_raw_train_(120, 290)\n",
    "no3_normalized_train_(120, 290)\n",
    "no3_min_max_train_(120, 290)\n",
    "no3_log_train_(120, 290)\n",
    "no3_k_scaled_train_(120, 290)\t\n",
    "si_normalized_train_(120, 290)\n",
    "si_min_max_train_(120, 290)\n",
    "si_log_train_(120, 290)\n",
    "si_k_scaled_train_(120, 290)\n",
    "intpp_lag1_train_(120, 290)\n",
    "intpp_lag1_raw_train_(120, 290)\n",
    "intpp_lag1_normalized_train_(120, 290)\n",
    "intpp_lag1_min_max_train_(120, 290)\n",
    "intpp_lag1_log_train_(120, 290)\n",
    "intpp_lag1_k_scaled_train_(120, 290)\"\"\"\n",
    "\n",
    "test_exp = f'{parent_dir}/plots/LSTM/MULTI_13210'\n",
    "n_forecast_steps = 60\n",
    "\n",
    "# Prepare input-output pairs for LSTM\n",
    "def create_dataset(data, look_back):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back), :])\n",
    "        Y.append(data[i + look_back, 0])  # Assuming predicting Y\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Function to compute RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Approach 1: Check if any NaN values exist in the DataFrame\n",
    "    has_nan_pred = np.isnan(y_pred).all()\n",
    "    has_nan_true = np.isnan(y_true).all()\n",
    "    \n",
    "    if has_nan_pred == True or has_nan_true == True:\n",
    "\n",
    "        print(f\"DataFrame contains NaN values y_pred: {has_nan_pred}\")\n",
    "\n",
    "        # Approach 2: Check for NaN values in each column and the entire DataFrame\n",
    "        nan_in_columns = y_pred.isnull().any()\n",
    "        print(\"NaN values in each column y_pred:\")\n",
    "        print(nan_in_columns)\n",
    "\n",
    "        nan_in_any = y_pred.isnull().any().any()\n",
    "        print(f\"DataFrame contains NaN valuesy_pred : {nan_in_any}\")\n",
    "\n",
    "        # Approach 3: Count NaN values in each column and the total count\n",
    "        nan_count = y_pred.isnull().sum()\n",
    "        print(\"Count of NaN values in each column y_pred:\")\n",
    "        print(nan_count)\n",
    "\n",
    "        total_nan_count = y_pred.isnull().sum().sum()\n",
    "        print(f\"Total count of NaN values in DataFrame y_pred: {total_nan_count}\")\n",
    "\n",
    "        print(f\"DataFrame contains NaN values y_true: {has_nan_true}\")\n",
    "\n",
    "        # Approach 2: Check for NaN values in each column and the entire DataFrame\n",
    "        nan_in_columns = y_true.isnull().any()\n",
    "        print(\"NaN values in each column y_true:\")\n",
    "        print(nan_in_columns)\n",
    "\n",
    "        nan_in_any = y_true.isnull().any().any()\n",
    "        print(f\"DataFrame contains NaN values y_true : {nan_in_any}\")\n",
    "\n",
    "        # Approach 3: Count NaN values in each column and the total count\n",
    "        nan_count = y_true.isnull().sum()\n",
    "        print(\"Count of NaN values in each column y_true:\")\n",
    "        print(nan_count)\n",
    "\n",
    "        total_nan_count = y_true.isnull().sum().sum()\n",
    "        print(f\"Total count of NaN values in DataFrame y_true: {total_nan_count}\")\n",
    "        \n",
    "\n",
    "    if has_nan_pred == False:\n",
    "        if has_nan_true == False:\n",
    "    \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            print(\"=\"*100)\n",
    "            print(\" RMSE FUNCTION \")\n",
    "            print(\"y_true\",y_true)\n",
    "            print(\"y_pred_transposed\",y_pred.transpose())\n",
    "            print(\"mean y_true\", np.mean(y_true))\n",
    "            print(\"mean y_pred\", np.mean(y_pred))\n",
    "            print(\"MSE :\", mse)\n",
    "            print(\"MAE :\", mae)\n",
    "            print(\"RMSE :\", rmse)\n",
    "            print(\"R2 score :\", r2)\n",
    "\n",
    "\n",
    "            rmse_percent = rmse/np.abs(np.mean(y_true))*100\n",
    "            \n",
    "            print(f\"rmse vs y_true {rmse: .2e} vs. {np.mean(y_true): .2e}\")\n",
    "            \n",
    "            print(\"MSE %:\", mse/np.mean(y_true)*100)\n",
    "            print(\"MAE %:\", mae/np.mean(y_true)*100)\n",
    "            print(\"RMSE %:\", rmse_percent)\n",
    "            print(\"=\"*100)\n",
    "    else:\n",
    "        \n",
    "        rmse_percent = np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    return rmse_percent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normx_dict = ['normalized','k_scaled']\n",
    "\n",
    "\n",
    "# Columns to add stepwise\n",
    "columns = ['A' ,'B','C','D', 'E', 'F', 'G', 'H', 'I','J','K','L']\n",
    "\n",
    "# DataFrame to store RMSE values\n",
    "rmse_results = pd.DataFrame(columns=['NORM', 'COLUMNS', 'VAR','RMSE','AREA','GEO','MODEL','HYPERTUNE','CORR','CORR_TRANSF'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mapping for train and test columns\n",
    "\n",
    "variables = {\n",
    "    'A': 'no3',\n",
    "    'B': 'po4',\n",
    "    'C': 'si',\n",
    "    'D': 'thetao',\n",
    "    'E': 'mlotst',\n",
    "    'F': 'dfe',\n",
    "    'G': 'zmicroos',\n",
    "    'H': 'zmesoos',\n",
    "    'I': 'rsntds',\n",
    "    'J': 'umo',\n",
    "    'K': 'vmo',\n",
    "    'L': 'wmo'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for area in area_dict:\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('RUN FOR FOLLOWING LOCATION ', area)\n",
    "    area = str(area)\n",
    "\n",
    "\n",
    "    for normx in normx_dict:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        train_columns = {\n",
    "            'A': 'no3_'+normx+'_train_'+area,\n",
    "            'B': 'po4_'+normx+'_train_'+area,\n",
    "            'C': 'si_'+normx+'_train_'+area,\n",
    "            'D': 'thetao_'+normx+'_train_'+area,\n",
    "            'E': 'mlotst_'+normx+'_train_'+area,\n",
    "            'F': 'dfe_'+normx+'_train_'+area,\n",
    "            'G': 'zmicroos_'+normx+'_train_'+area,\n",
    "            'H': 'zmesoos_'+normx+'_train_'+area,\n",
    "            'I': 'rsntds_'+normx+'_train_'+area,\n",
    "            'J': 'umo_'+normx+'_train_'+area,\n",
    "            'K': 'vmo_'+normx+'_train_'+area,\n",
    "            'L': 'wmo_'+normx+'_train_'+area\n",
    "        }\n",
    "\n",
    "        test_columns = {\n",
    "            'A': 'no3_'+normx+'_test_'+area,\n",
    "            'B': 'po4_'+normx+'_test_'+area,\n",
    "            'C': 'si_'+normx+'_test_'+area,\n",
    "            'D': 'thetao_'+normx+'_test_'+area,\n",
    "            'E': 'mlotst_'+normx+'_test_'+area,\n",
    "            'F': 'dfe_'+normx+'_test_'+area,\n",
    "            'G': 'zmicroos_'+normx+'_test_'+area,\n",
    "            'H': 'zmesoos_'+normx+'_test_'+area,\n",
    "            'I': 'rsntds_'+normx+'_test_'+area,\n",
    "            'J': 'umo_'+normx+'_test_'+area,\n",
    "            'K': 'vmo_'+normx+'_test_'+area,\n",
    "            'L': 'wmo_'+normx+'_test_'+area\n",
    "        }\n",
    "\n",
    "\n",
    "        #normx = normx_dict[i]\n",
    "\n",
    "        df_test_predictor = df_test['intpp_'+normx+'_test_'+area][0:]\n",
    "        df_train_predictor_shift = df_train['intpp_'+normx+'_train_'+area][1:]\n",
    "        df_train_predictor = df_train['intpp_'+normx+'_train_'+area][0:]\n",
    "\n",
    "        # Create new DataFrames for dependent and independent variables for multivariate of Z, A, B\n",
    "\n",
    "        df_new = pd.DataFrame(df_train_predictor, columns=['Z'])\n",
    "\n",
    "        df_new['Z'] = df_train_predictor_shift\n",
    "        df_new['Zt-1'] = df_train['intpp_lag1_'+normx+'_train_'+area][1:]\n",
    "\n",
    "        #print(df_new.head())\n",
    "\n",
    "\n",
    "        # Create new DataFrames for dependent and independent variables for multivariate of Z, A, B\n",
    "        df_new_test = pd.DataFrame(df_test_predictor, columns=['Z'])\n",
    "        df_new_test['Z'] = df_test_predictor\n",
    "        df_new_test['Zt-1'] = df_test['intpp_lag1_'+normx+'_test_'+area]\n",
    "\n",
    "\n",
    "        # Histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(df_new['Z'])\n",
    "        plt.hist(df_new['Zt-1'])\n",
    "        plt.title(f'ML LSTM Multvar. INTPP  {normx.upper()} @ {area}')\n",
    "        plt.xlabel('Data value')\n",
    "        plt.ylabel('#N')\n",
    "        plt.legend('INTPP')\n",
    "        \n",
    "        print(\"Root Path for Experiment: \",test_exp)\n",
    "        print(\"os.path.exists(test_exp)???\",os.path.exists(test_exp))\n",
    "        if os.path.exists(test_exp) == False:\n",
    "            print(\"!mkdir 'plots/LSTM/MULTI_'+test_exp\")\n",
    "            os.makedirs(test_exp, exist_ok=True)\n",
    "            print(\"os.path.exists(test_exp)!!!\",os.path.exists(test_exp))\n",
    "        \n",
    "        if os.path.exists('plots/LSTM/MULTI_'+test_exp+'/HIST') == False:\n",
    "            os.makedirs(test_exp+'/HIST', exist_ok=True) \n",
    "        plt.savefig(test_exp+f'/HIST/HISTOGRAM_var_0_INTPP_{normx}_{area}.png') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Iterate through columns and expand the DataFrame stepwise\n",
    "        for col in columns:\n",
    "            n_var = str(columns.index(col)+1)\n",
    "            df_new[col] = df_train[train_columns[col]][1:]\n",
    "            df_new_test[col] = df_test[test_columns[col]]\n",
    "            \n",
    "            df_new.fillna(0)\n",
    "            df_new_test.fillna(0)\n",
    "            # Evaluate the DataFrame at each step (you can add your own evaluation code here)\n",
    "            print(f\"Evaluating with columns: ['Z', 'Zt-1'] + {columns[:columns.index(col)+1]}\")\n",
    "            #print(df_new.head())\n",
    "            #print(df_new_test.head())\n",
    "\n",
    "        \n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(df_new)\n",
    "            scaled_test_data = scaler.fit_transform(df_new_test)\n",
    "\n",
    "\n",
    "\n",
    "            print(np.shape(scaled_data))\n",
    "            print(np.shape(scaled_test_data))\n",
    "\n",
    "            # Validation and forecasting steps omitted for brevity\n",
    "\n",
    "\n",
    "\n",
    "            look_back = 12  # Number of time steps to look back\n",
    "            X, Y = create_dataset(scaled_data, look_back)\n",
    "\n",
    "            # Define and train the LSTM model\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(units=50, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "            # Add dense layers\n",
    "            model.add(Dense(units=32, activation='relu'))\n",
    "            model.add(Dense(units=1, activation='linear'))  # Assuming you are predicting a single output\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(units=1))\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            # Train the model and store the training history\n",
    "            from keras.callbacks import EarlyStopping\n",
    "            # Define the EarlyStopping callback\n",
    "            #early_stopping = EarlyStopping(monitor='loss', patience=8, verbose=1)\n",
    "\n",
    "            # Fit the model with EarlyStopping callback\n",
    "            history = model.fit(X, Y, epochs=100, batch_size=32, verbose=0)#, validation_split=0.2)\n",
    "            #history = model.fit(X, Y, epochs=100, batch_size=32,callbacks=[early_stopping])\n",
    "\n",
    "            # Access the loss values from the history object\n",
    "            loss = history.history['loss']\n",
    "            #acc = history.history['accuracy']\n",
    "            # Plot the loss curve\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(len(loss)), loss)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss: Mean Squared Error')\n",
    "            plt.title(f'Loss_{n_var}_INTPP_to_{variables[col]}_{normx}_{area}')\n",
    "            if os.path.exists(test_exp+'/LOSS') == False:\n",
    "                os.makedirs(test_exp+'/LOSS', exist_ok=True) \n",
    "            plt.savefig(test_exp+f'/LOSS/LOSS_{n_var}_INTPP_to_{variables[col]}_{normx}_{area}.png') \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "            # Transform the values into the scaled 0..1\n",
    "            scaled_test_data = scaler.fit_transform(df_new_test)\n",
    "\n",
    "\n",
    "            X_test, Y_test = create_dataset(scaled_test_data, look_back)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #How many steps do you want predict?\n",
    "\n",
    "            X_test = X_test[0:n_forecast_steps-12]\n",
    "            Y_test = Y_test[0:n_forecast_steps-12]\n",
    "\n",
    "\n",
    "\n",
    "            # Given minimum and maximum values per feature for the first column\n",
    "            min_val = scaler.data_min_[0]\n",
    "            max_val = scaler.data_max_[0]\n",
    "\n",
    "\n",
    "            # Generate predictions on the scaled test data\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            y_pred_transformed = y_pred*(max_val - min_val)+min_val\n",
    "            Y_test_transformed = Y_test*(max_val - min_val)+min_val\n",
    "             # Calculate RMSE\n",
    "            rmse = compute_rmse(Y_test_transformed, y_pred_transformed)\n",
    "            print(\"RMSE Transfomred FLAT\", rmse)\n",
    "            # Histogram\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(df_new[col])\n",
    "            plt.title(f' Histogram {variables[col].upper()} & {normx.upper()} @ {area} forecast. RMSE:= {rmse: .2f} %')\n",
    "            plt.xlabel('Data value')\n",
    "            plt.ylabel('#N')\n",
    "            print(\"variables[col].upper()\",variables[col].upper())\n",
    "            plt.legend(variables[col].upper())\n",
    "            if os.path.exists(test_exp+'/HIST') == False:\n",
    "                os.makedirs(test_exp+'/HIST', exist_ok=True) \n",
    "            plt.savefig(test_exp+f'/HIST/HISTOGRAM_var_{variables[col]}_{normx}_{area}.png') \n",
    "            \n",
    "\n",
    "            \n",
    "           \n",
    "            \n",
    "            y_pred_flat = y_pred.flatten()\n",
    "            \n",
    "       \n",
    "            \n",
    "            y_pred_transformed_flat = y_pred_transformed.flatten()\n",
    "            \n",
    "            \n",
    "            \n",
    "            corr_forecast_orig = apply_cross_correlation(y_pred_flat,Y_test)\n",
    "          \n",
    "            corr_forecast_transformed = apply_cross_correlation(y_pred_transformed_flat,Y_test_transformed)\n",
    "    \n",
    "            \n",
    "            \n",
    "           \n",
    "            import ast\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = compute_rmse(Y_test_transformed, y_pred_transformed_flat)\n",
    "            print(\"RMSE Transfomred FLAT\", rmse)\n",
    "  \n",
    "            \n",
    "            # Store the RMSE value in the DataFrame\n",
    "            new_row = pd.DataFrame({'NORM': [normx], 'COLUMNS': [columns[:columns.index(col)+1]],\n",
    "                                    'VAR': [variables[col]],\n",
    "                                    'RMSE': [rmse],'AREA': [area],'GEO': geo_locations['geo_points'][ast.literal_eval(area)],'MODEL': 'LSTM','HYPERTUNE':'50,32,0.1,1,1',\n",
    "                                   'CORR': [corr_forecast_orig],'CORR_TRANSF': [corr_forecast_transformed]}\n",
    "                                  )\n",
    "            rmse_results = pd.concat([rmse_results, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            # Plot the actual values and the predictions\n",
    "            plt.plot(Y_test_transformed, label='Test sequence',linestyle='dotted')\n",
    "            plt.plot(y_pred_transformed, label='Predicted sequence')\n",
    "            plt.title(f'ML LSTM Multvar. INTPP to {variables[col].upper()} &  {normx.upper()} @ {area} forecast. RMSE:= {rmse: .2f} %')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Re-Scaled values [ ]')\n",
    "            plt.legend()\n",
    "            if os.path.exists(test_exp+'/FORECAST') == False:\n",
    "                os.makedirs(test_exp+'/FORECAST', exist_ok=True) \n",
    "            \n",
    "            plt.savefig(test_exp+f'/FORECAST/FORECAST_var_{n_var}_INTPP_to_{variables[col]}_{normx}_{area}.png') \n",
    "\n",
    "            #plt.show()\n",
    "\n",
    "# Save the RMSE results to a CSV file\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y-%m-%d_%HH:%MM:%SS\")\n",
    "\n",
    "rmse_results.to_csv(test_exp+f'/LSTM_RMSE_{stamp}.csv', index=False)  \n",
    "\n",
    "\n",
    "\n",
    "df = rmse_results\n",
    "\n",
    "# Separate heatmaps for 'normalized' and 'k_scaled'\n",
    "for norm_type in df['NORM'].unique():\n",
    "    # Filter the DataFrame by 'NORM' value\n",
    "    df_filtered = df[df['NORM'] == norm_type]\n",
    "    \n",
    "    # Pivot the table to get 'AREA' on the y-axis, 'VAR' on the x-axis, and 'CORR' as values\n",
    "    heatmap_data = df_filtered.pivot(index='AREA', columns='VAR', values='CORR')\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\",\n",
    "                vmin=-1,vmax=1,\n",
    "                cbar_kws={'label': 'Correlation'})\n",
    "    plt.title(f'Corr. Heatmap of Area vs Variables for {norm_type} from LSTM model')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.ylabel('Area')\n",
    "    \n",
    "    # Save the heatmap\n",
    "    plt.savefig(test_exp+f'/Heatmap_LSTM_CORR_{norm_type}_INDEX.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    heatmap_data = df_filtered.pivot(index='GEO', columns='VAR', values='CORR')\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\",\n",
    "                vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'})\n",
    "    plt.title(f'Corr. Heatmap of Geolocation vs Variables for {norm_type} from LSTM model')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.ylabel('Geolocation')\n",
    "    \n",
    "    # Save the heatmap\n",
    "    plt.savefig(test_exp+f'/Heatmap_LSTM_CORR_{norm_type}_GEO.png')\n",
    "\n",
    "\n",
    "    # Pivot the table to get 'AREA' on the y-axis, 'VAR' on the x-axis, and 'CORR' as values\n",
    "    heatmap_data = df_filtered.pivot(index='AREA', columns='VAR', values='RMSE')\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\",\n",
    "                vmin=0,vmax=20,cbar_kws={'label': 'RMSE %'})\n",
    "    plt.title(f'RMSE Heatmap of Area vs Variables for {norm_type} from LSTM model')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.ylabel('Area')\n",
    "    \n",
    "    # Save the heatmap\n",
    "    plt.savefig(test_exp+f'/Heatmap_LSTM_RMSE_{norm_type}_INDEX.png')\n",
    "    \n",
    "    # Pivot the table to get 'AREA' on the y-axis, 'VAR' on the x-axis, and 'CORR' as values\n",
    "    heatmap_data = df_filtered.pivot(index='GEO', columns='VAR', values='RMSE')\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\",\n",
    "                vmin=0,vmax=20,cbar_kws={'label': 'RMSE %'})\n",
    "    plt.title(f'RMSE Heatmap of Area vs Variables for {norm_type} from LSTM model')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.ylabel('Area')\n",
    "    \n",
    "    # Save the heatmap\n",
    "    plt.savefig(test_exp+f'/Heatmap_LSTM_RMSE_{norm_type}_GEO.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "end_time_lstm = time.time()\n",
    "\n",
    "time_used_lstm = end_time_lstm - start_time_lstm\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "print(\"Time used for LSTM ..../zzzz\", f'{time_used_lstm:.4f}', \"Seconds\")\n",
    "print(\"Time used for LSTM..../zzzz\", f'{time_used_lstm/60:.4f}', \"Minutes\")\n",
    "print(\"Time used for LSTM..../zzzz\", f'{time_used_lstm/3600:.4f}', \"Hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fd245",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook:.4f}', \"Seconds\")\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load:.4f}', \"Seconds\")\n",
    "#print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot:.4f}', \"Seconds\")\n",
    "#print(\"Time used for entire PLOTSERIES\", f'{time_used_plot1:.4f}', \"Seconds\")\n",
    "print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor:.4f}', \"Seconds\")\n",
    "\n",
    "print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm:.4f}', \"Seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook/60:.4f}', \"Minutes\")\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/60:.4f}', \"Minutes\")\n",
    "#print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot/60:.4f}', \"Minutes\")\n",
    "#print(\"Time used for entire PLOTSERIES\", f'{time_used_plot1/60:.4f}', \"Minutes\")\n",
    "print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor/60:.4f}', \"Minutes\")\n",
    "\n",
    "print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm/60:.4f}', \"Minutes\")\n",
    "\n",
    "print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook/3600:.4f}', \"Hours\")\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/3600:.4f}', \"Hours\")\n",
    "#print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot/3600:.4f}', \"Hours\")\n",
    "#print(\"Time used for entire PLOTSERIES\", f'{time_used_plot1/3600:.4f}', \"Hours\")\n",
    "print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor/3600:.4f}', \"Hours\")\n",
    "\n",
    "print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm/3600:.4f}', \"Hours\")\n",
    "\n",
    "print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook/time_used_notebook*100:.2f}', \"%\")\n",
    "print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/time_used_notebook*100:.2f}', \"%\")\n",
    "#print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot/time_used_notebook*100:.2f}', \"%\")\n",
    "#print(\"Time used for entire PLOT SERIES..../zzzz\", f'{time_used_plot1/time_used_notebook*100:.2f}', \"%\")\n",
    "print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor/time_used_notebook*100:.2f}', \"%\")\n",
    "\n",
    "print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm/time_used_notebook*100:.2f}', \"%\")\n",
    "\n",
    "\n",
    "# Open a file in write mode\n",
    "with open(f\"{parent_dir}/output_time/Heatmap_{epoch_range}/output_time_heatmap_{epoch_range}_overview_1.txt\", \"w\") as file:\n",
    "    # Redirect print output to the file\n",
    "    print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook:.4f}', \"Seconds\", file=file)\n",
    "    print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load:.4f}', \"Seconds\", file=file)\n",
    "    #print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot:.4f}', \"Seconds\")\n",
    "    #print(\"Time used for entire PLOTSERIES\", f'{time_used_plot1:.4f}', \"Seconds\")\n",
    "    print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor:.4f}', \"Seconds\", file=file)\n",
    "\n",
    "    print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm:.4f}', \"Seconds\", file=file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook/60:.4f}', \"Minutes\", file=file)\n",
    "    print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/60:.4f}', \"Minutes\", file=file)\n",
    "    #print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot/60:.4f}', \"Minutes\")\n",
    "    #print(\"Time used for entire PLOTSERIES\", f'{time_used_plot1/60:.4f}', \"Minutes\")\n",
    "    print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor/60:.4f}', \"Minutes\", file=file)\n",
    "\n",
    "    print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm/60:.4f}', \"Minutes\", file=file)\n",
    "\n",
    "    print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook/3600:.4f}', \"Hours\", file=file)\n",
    "    print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/3600:.4f}', \"Hours\", file=file)\n",
    "    #print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot/3600:.4f}', \"Hours\")\n",
    "    #print(\"Time used for entire PLOTSERIES\", f'{time_used_plot1/3600:.4f}', \"Hours\")\n",
    "    print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor/3600:.4f}', \"Hours\", file=file)\n",
    "\n",
    "    print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm/3600:.4f}', \"Hours\", file=file)\n",
    "\n",
    "    print(\"Time used for entire NOTEBOOK..../zzzz\", f'{time_used_notebook/time_used_notebook*100:.2f}', \"%\", file=file)\n",
    "    print(\"Time used for entire LOAD..../zzzz\", f'{time_used_load/time_used_notebook*100:.2f}', \"%\", file=file)\n",
    "    #print(\"Time used for entire PLOT2D..../zzzz\", f'{time_used_plot/time_used_notebook*100:.2f}', \"%\", file=file)\n",
    "    #print(\"Time used for entire PLOT SERIES..../zzzz\", f'{time_used_plot1/time_used_notebook*100:.2f}', \"%\")\n",
    "    print(\"Time used for entire EXTRACTOR.../zzzz\", f'{time_used_extractor/time_used_notebook*100:.2f}', \"%\", file=file)\n",
    "\n",
    "    print(\"Time used for entire LSTM.../zzzz\", f'{time_used_lstm/time_used_notebook*100:.2f}', \"%\", file=file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817380f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711db43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9f36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faccbf0-f0e5-4ea8-86bb-813cc1552074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
